\section{Discussion}
\label{sec:discussion}

In this section, we discuss the implications of the results for
researchers and practitioners as well as the limitations of
recommender systems that we found during our experiment.

\vspace*{2pt}
\subsubsection*{Implications of the Results}

Our experiment results indicate that by utilizing a recommender system
when we prioritize test cases, we were able to improve the effectiveness 
of test case prioritization. Further, we found that our proposed approach
is far more effective than the control techniques when testers have a 
limited time budget during testing. 
In particular, from the RQ2 results, we learned that our approach produced
much greater benefits when the time budget is limited.
The findings from this study could help practitioners select appropriate {\large }
test prioritization techniques considering their product delivery schedule   
and other circumstances (e.g., the availability of required software attributes). 

%We also found that the results showed some differences between two types 
%of applications: proprietary and open source applications.
%When we compared {\em DASCP}, which is a proprietary application, with two  
%open source application, we noticed that there is less data variance in {\em DASCP}.
%We examined our dataset to investigate the reason for this difference 
%and we found that there is a higher cohesion between user interaction data
%in {\em DASCP} than in the two open source applications.
%We speculate that this difference came from the types of users who use the applications.
%{\em DASCP} users were actual users who have domain knowledge of the application, 
%but for the open source applications, the users were student participants whose
%usage patterns were not coherent.
%This result indicates that the use of actual user data could produce more
%stable results than using volunteer users, and this finding should be further 
%investigated through additional experiments with actual user datasets. 


While we have not conducted a cost benefit analysis of our approach,
which is our future work, we found that the cost of applying our approach is
negligible. The biggest cost for applying our approach is calculating the matrix
of access frequency scores, but it took only 580, 153, and 374 seconds
for {\em nopCommerce}, {\em Coevery} and {\em DASCP}, respectively.  
Further, the prioritization algorithm took only 0.29 seconds for {\em nopCommerce},
the application that has the largest number of test cases.

\vspace*{1pt}
\subsubsection*{Limitations of Applying Recommender Systems}

While our results indicate that recommender systems can 
improve test case prioritization, like any other approaches, recommender 
systems also have their own limitations. 
In this study, we applied an item-based collaborative
filtering algorithm that calculates the components' similarity based
on user ratings. This means that our results are highly dependent on 
the accuracy of user ratings, and thus user ratings can greatly affect 
the experiment's results.
There are three common limitations in collaborative filtering recommender systems;
new user problem, new item problem, and sparsity~\cite{recomsurvey05}. 
Among these three limitations, two of them are related to our study. 

%\begin{smallitem}
%	\item
	
	{\textbf{Sparsity.}}	
	As we discussed earlier, we do not have a rating module in our
	applications, and thus we used access frequencies to each component
	as a user rating. Further, for the open source applications, {\em nopCommerce} 
    and {\em Coevery}, we collected the user interaction data
	from nonprofessional users, so our data could contain noise and 
	redundancy, which can affect the performance of our technique. 
	Moreover, the number of collected ratings is relatively small, compared 
	to the number of expected ratings required to generate an accurate prediction.
	
	Also, the distribution patterns of user ratings can affect the outcome
    of collaborative filtering algorithms. For example, in {\em nopCommerce} case, 
	some components were used by all users,
	such as registration and membership components, while some other
	components were ignored by the majority of users. 
	In order to eliminate this issue, we need to collect more user 
    interaction data by considering a larger number of users and a longer 
    period of time.
%    which to monitor user interactions.
	In addition, having actual users would generate more realistic results, because
    their interactions would be based on real business functions and system workflows.
	
%	\item
	{\textbf{New Item Problem.}}	
	Another limitation of collaborative filtering that is related to
	our study is the ``New Item Problem''. It is a common practice that
	newly developed components are frequently added to a system. 
	However, rankings on collaborative filtering algorithms are 
        based on user access frequencies to the components. Therefore, 
        the newly added components would not appear in the recommender suggestion 
        list until a certain number of users perform some tasks on them. 
	In our study, in addition to using access frequency scores, 
    we also applied change risk scores to recommend the riskiest components.
	Therefore, even if a new component has a high change risk score,
    its frequency score would still be zero, which makes the overall 
    risk score zero. To overcome this limitation, we need
	to use a hybrid and normalized ranking score by assigning 
    a small value to the components.
%\end{smallitem}


 

