\section{Discussion}
\label{sec:discussion}

In this section, we discuss the implications of results for
researchers and practitioners as well as the limitations of
recommender systems that we found during our experiment.

\vspace*{2pt}
\subsubsection*{Implications of the Results}

Our experiment results indicate that by utilizing a recommender system
when we prioritize test cases, we were able to improve the effectiveness 
of test case prioritization. Further, we found that our proposed approach
is far more effective than the control techniques when testers have a 
limited time budget during testing. 
In particular, from the RQ2 results, we learned that our approach produced
much greater benefits when the time budget is limited.
The findings from this study could help practitioners select appropriate {\large }
test prioritization techniques considering their product delivery schedule   
and other circumstances (e.g., the availability of required software attributes). 

We also found that the results showed some differences between two types 
of applications: proprietary and open source applications.
When we compared DASCP, which is a proprietary application, with two  
open source application, we noticed that there is less data variance in DASCP.
We examined our dataset to investigate the reason for this difference 
and we found that there is a higher cohesion between user interaction data
in DASCP than in the two open source applications.
We speculate that this difference came from the types of users who use the applications.
DASCP users were actual users who have domain knowledge of the application, 
but for the open source applications, the users were student participants whose
usage patterns were not coherent.
This result indicates that the use of actual user data could produce more
stable results than using volunteer testers, and this finding should be further 
investigated through additional experiments with actual user datasets. 

\vspace*{2pt}
\subsubsection*{Limitations of Applying Recommender Systems}

While the results of our study indicate that recommender systems can help
improve test case prioritization, like any other approaches, recommender 
systems also have their own limitations. 
In this study, we applied an item-based collaborative
filtering algorithm that calculates the components' similarity based
on user ratings. This means that our results are highly dependent on 
the accuracy of user ratings, and thus user ratings can greatly affect 
the experiment's results.
There are three common limitations in collaborative filtering recommender systems;
new user problem, new item problem, and sparsity~\cite{recomsurvey05}. 
Among these three limitations, two of them are related to our study. 

\begin{itemize}
	\item{\textbf{Sparsity.}}	
	As we discussed earlier, we do not have a rating module in our
	applications, and thus we used access frequencies to each component
	as a user rating. Further, for the open source applications, nopCommerce 
    and Coevery, we collected the user interaction data
	by asking non-professional users, so our data could contain noise and 
	redundancy, which can affect the performance of our technique. 
	Moreover, the number of collected ratings is relatively small, compared 
	to the number of expected ratings to have an accurate prediction.
	
	Also, the distribution patterns of user ratings can affect the outcome
    of collaborative filtering algorithms. For example, in our case, 
	some components were used by all users,
	such as registration and membership components, while some other
	components were ignored by the majority of users. 
	In order to eliminate this issue, we need to collect more user 
        interaction data by considering a larger number of users and a longer 
        period of time during which to monitor user interactions.
	Further, having actual users would provide more realistic results, because
        their interactions would be based on real business functions and system workflow.
	
	\item{\textbf{New Item Problem.}}	
	Another limitation of collaborative filtering that is related to
	our study is the ``New Item Problem''. It is a common practice that
	newly developed components are frequently added to a system. 
	However, rankings on collaborative filtering algorithms are 
        based on user access frequencies to the components. Therefore, 
        the newly added components would not be in the recommender suggestion 
        list until a certain number of users perform some tasks on them. 
	In our study, in addition to using access frequency scores, 
        we also applied change risk scores to recommend the highest risky components.
	Therefore, even if a new component has a high change risk score,
         its frequency score would still be zero, which makes the overall 
        risk score zero. To overcome this limitation, we need
	to use a hybrid and normalized ranking score by assigning 
        a small value to the components.
\end{itemize}
